{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# IRGAN: Generative Adversarial Nets for Information Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook provides the implimentations of [IRGAN](https://arxiv.org/pdf/1705.10513.pdf) for a learning to rank of a pairwise approach published on SIGIR 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Copyright (C) 2017 Takahiro Ishikawa  \n",
    "  \n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  \n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  \n",
    "  \n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset (MQ2008-semi)\n",
    "\n",
    "You should download the data set from [here](https://drive.google.com/drive/folders/0B-dulzPp3MmCM01kYlhhNGQ0djA?usp=sharing), and then put in MQ2008-semi/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MQ2008NoTargetException(Exception):\n",
    "    pass\n",
    "\n",
    "class MQ2008:\n",
    "    def __init__(self, dataset_dir='MQ2008-semi'):\n",
    "        self.pool = {}\n",
    "        self.pool['train'] = self._load_data(dataset_dir + '/train.txt')\n",
    "        self.pool['test'] = self._load_data(dataset_dir + '/test.txt')\n",
    "        \n",
    "    def get_queries(self, target='train'):\n",
    "        if target in self.pool.keys():\n",
    "            return list(self.pool[target].keys())\n",
    "        else:\n",
    "            raise MQ2008NoTargetException()\n",
    "\n",
    "    def get_docs(self, query, target='train'):\n",
    "        if target in self.pool.keys():\n",
    "            return list(self.pool[target][query].keys())\n",
    "        else:\n",
    "            raise MQ2008NoTargetException()\n",
    "    \n",
    "    def get_features(self, query, doc, target='train'):\n",
    "        if target in self.pool.keys():\n",
    "            return self.pool[target][query][doc]['f']\n",
    "        else:\n",
    "            raise MQ2008NoTargetException()\n",
    "    \n",
    "    def get_rank(self, query, doc, target='train'):\n",
    "        if target in self.pool.keys():\n",
    "            return self.pool[target][query][doc]['r']\n",
    "        else:\n",
    "            raise MQ2008NoTargetException()\n",
    " \n",
    "    def get_pos_queries(self, target='train'):\n",
    "        if target in self.pool.keys():\n",
    "            return list({query for query in self.get_queries(target=target)\n",
    "                         for doc in self.get_docs(query, target=target) if self.get_rank(query, doc, target=target) > 0.0})\n",
    "        else:\n",
    "            raise MQ2008NoTargetException()\n",
    "            \n",
    "    def get_pos_docs(self, query, target='train'):\n",
    "        if target in self.pool.keys():\n",
    "            return list({doc for doc in self.get_docs(query, target=target) if self.get_rank(query, doc, target=target) > 0.0})\n",
    "        else:\n",
    "            raise MQ2008NoTargetException()\n",
    "\n",
    "    # load docs and features for a query.\n",
    "    def _load_data(self, file, feature_size=46):\n",
    "        query_doc_feature = {}\n",
    "        with open(file) as f:\n",
    "            for line in f:\n",
    "                cols = line.strip().split()\n",
    "                rank = cols[0]\n",
    "                query = cols[1].split(':')[1]\n",
    "                doc = cols[-7]\n",
    "                feature = []\n",
    "                for i in range(2, 2 + feature_size):\n",
    "                    feature.append(float(cols[i].split(':')[1]))\n",
    "                if query in query_doc_feature.keys():\n",
    "                    query_doc_feature[query][doc] = {'r': float(rank), 'f': np.array(feature)}\n",
    "                else:\n",
    "                    query_doc_feature[query] = {doc: {'r': float(rank), 'f': np.array(feature)}}\n",
    "        return query_doc_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here we need to use `tf.variable_scope` for two reasons. Firstly, we're going to make sure all the variable names start with `generator`. Similarly, we'll prepend `discriminator` to the discriminator variables. This will help out later when we're training the separate networks.\n",
    "\n",
    "Here's more from [the TensorFlow documentation](https://www.tensorflow.org/programmers_guide/variable_scope#the_problem) to get another look at using `tf.variable_scope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, feature_size, hidden_size, keep_prob=1.0):\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "        with tf.variable_scope('generator'):\n",
    "            # input placeholders\n",
    "            self.reward = tf.placeholder(tf.float32, [None], name='reward')\n",
    "            self.pred_data = tf.placeholder(tf.float32, [None, self.feature_size], name='pred_data')\n",
    "            self.sample_index = tf.placeholder(tf.int32, [None], name='sample_index')\n",
    "            \n",
    "            ########## score of RankNet ##########\n",
    "\n",
    "            # trainable variables\n",
    "            self.weight_1 = tf.Variable(tf.truncated_normal([self.feature_size, self.hidden_size], mean=0.0, stddev=0.1), name='weight_1')\n",
    "            self.bias_1 = tf.Variable(tf.zeros([self.hidden_size]), name='bias_1')\n",
    "            self.weight_2 = tf.Variable(tf.truncated_normal([self.hidden_size, 1], mean=0.0, stddev=0.1), name='weight_2')\n",
    "            self.bias_2 = tf.Variable(tf.zeros([1]), name='bias_2')\n",
    "\n",
    "            # layer 1 (hidden layer)\n",
    "            self.layer_1 = tf.nn.tanh(tf.nn.xw_plus_b(self.pred_data, self.weight_1, self.bias_1))\n",
    "            \n",
    "            # dropout\n",
    "            self.dropout_1 = tf.nn.dropout(self.layer_1, self.keep_prob)\n",
    "\n",
    "            # layer 2 (output layer)\n",
    "            self.layer_2 = tf.nn.xw_plus_b(self.dropout_1, self.weight_2, self.bias_2)\n",
    "            \n",
    "            #################################\n",
    "            \n",
    "            # probability distribution\n",
    "            self.opt_prob =tf.gather(tf.reshape(tf.nn.softmax(tf.reshape(self.layer_2, [1, -1])), [-1]), self.sample_index)\n",
    "            \n",
    "            # loss for optimization\n",
    "            self.opt_loss = -tf.reduce_mean(tf.log(self.opt_prob) * self.reward) # minus signe is needed for maximum.\n",
    "            \n",
    "            # score for prediction\n",
    "            self.pred_score = tf.nn.xw_plus_b(self.layer_1, self.weight_2, self.bias_2)\n",
    "            self.pred_score = tf.reshape(self.pred_score, [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Discriminator\n",
    "\n",
    "This is a pairwaise case implimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, feature_size, hidden_size, keep_prob=1.0):\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "        with tf.variable_scope('discriminator'):\n",
    "            # input placeholders\n",
    "            self.pos_data = tf.placeholder(tf.float32, [None, self.feature_size], name='pos_data')\n",
    "            self.neg_data = tf.placeholder(tf.float32, [None, self.feature_size], name='neg_data')\n",
    "            self.pred_data = tf.placeholder(tf.float32, [None, self.feature_size], name='pred_data')\n",
    "\n",
    "            ########## score of RankNet ##########\n",
    "            \n",
    "            ## trainable variables\n",
    "            self.weight_1 = tf.Variable(tf.truncated_normal([self.feature_size, self.hidden_size], mean=0.0, stddev=0.1), name='weight_1')\n",
    "            self.bias_1 = tf.Variable(tf.zeros([self.hidden_size]), name='bias_1')\n",
    "            self.weight_2 = tf.Variable(tf.truncated_normal([self.hidden_size, 1], mean=0.0, stddev=0.1), name='weight_2')\n",
    "            self.bias_2 = tf.Variable(tf.zeros([1]), name='bias_2')\n",
    "            \n",
    "            # layer 1 (hidden layer)\n",
    "            self.pos_layer_1 = tf.nn.tanh(tf.nn.xw_plus_b(self.pos_data, self.weight_1, self.bias_1))\n",
    "            self.neg_layer_1 = tf.nn.tanh(tf.nn.xw_plus_b(self.neg_data, self.weight_1, self.bias_1))\n",
    "            \n",
    "            # dropout\n",
    "            self.pos_dropout_1 = tf.nn.dropout(self.pos_layer_1, self.keep_prob)\n",
    "            self.neg_dropout_1 = tf.nn.dropout(self.neg_layer_1, self.keep_prob)\n",
    "            \n",
    "            # layer 2 (output layer)\n",
    "            self.pos_layer_2 = tf.nn.xw_plus_b(self.pos_dropout_1, self.weight_2, self.bias_2)\n",
    "            self.neg_layer_2 = tf.nn.xw_plus_b(self.neg_dropout_1, self.weight_2, self.bias_2)\n",
    "            \n",
    "            #################################\n",
    "            \n",
    "            # loss for optimization\n",
    "            self.opt_loss = -tf.reduce_mean(tf.log(tf.sigmoid(self.pos_layer_2 - self.neg_layer_2))) # minus signe is needed for miximum\n",
    "            \n",
    "            # reward for generator\n",
    "            self.reward = tf.reshape(tf.log(1 + tf.exp(self.neg_layer_2 - self.pos_layer_2)), [-1])\n",
    "            \n",
    "            # score for prediction\n",
    "            self.pred_score = tf.nn.xw_plus_b(tf.nn.tanh(tf.nn.xw_plus_b(self.pred_data, self.weight_1, self.bias_1)), self.weight_2, self.bias_2)\n",
    "            self.pred_score = tf.reshape(self.pred_score , [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimizer\n",
    "\n",
    "We want to update the generator and discriminator variables separately. So we need to get the variables for each part build optimizers for the two parts. To get all the trainable variables, we use `tf.trainable_variables()`. This creates a list of all the variables we've defined in our graph.\n",
    "\n",
    "For the generator optimizer, we only want to generator variables. Our past selves were nice and used a variable scope to start all of our generator variable names with `generator`. So, we just need to iterate through the list from `tf.trainable_variables()` and keep variables to start with `generator`. Each variable object has an attribute `name` which holds the name of the variable as a string (`var.name == 'weights_0'` for instance). \n",
    "\n",
    "We can do something similar with the discriminator. All the variables in the discriminator start with `discriminator`.\n",
    "\n",
    "Then, in the optimizer we pass the variable lists to `var_list` in the `minimize` method. This tells the optimizer to only update the listed variables. Something like `tf.train.AdamOptimizer().minimize(loss, var_list=var_list)` will only train the variables in `var_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, g, d, learning_rate):\n",
    "        # get the trainable_variables, split into generator and discriminator parts.\n",
    "        t_vars = tf.trainable_variables()\n",
    "        self.g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "        self.d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "\n",
    "        self.g_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(g.opt_loss, var_list=self.g_vars)        \n",
    "        self.d_train_opt = tf.train.AdamOptimizer(learning_rate).minimize(d.opt_loss, var_list=self.d_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Dataset (MQ2008-semi extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If there are only two levels of relevance and for each \"observed\" relevant-irrelevant document pair (d_i, d_j) we sample an unlabelled document d_k to form the \"generated\" document pair (d_k, d_j), then it can be shown that the objective function of the IRGAN-pairwise minimax game Eq. (7) in the paper is bounded by the mathematical expectation of (f_phi(d_i, q) - f_phi(d_k, q)) / 2 which is independent of the irrelevant document d_j, via a straightforward application of Jensen's inequality on the logarithm function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dataset(MQ2008):\n",
    "    def __init__(self, batch_size, dataset_dir='MQ2008-semi'):\n",
    "        MQ2008.__init__(self, dataset_dir=dataset_dir)\n",
    "        self.batch_size = batch_size\n",
    "        self.docs_pairs = []\n",
    "    \n",
    "    def set_docs_pairs(self, sess, generator):\n",
    "        for query in dataset.get_pos_queries():\n",
    "            can_docs = dataset.get_docs(query)\n",
    "            can_features = [dataset.get_features(query, doc) for doc in can_docs]\n",
    "            can_score = sess.run(generator.pred_score, feed_dict={generator.pred_data: can_features})\n",
    "        \n",
    "            # softmax for candidate\n",
    "            exp_rating = np.exp(can_score)\n",
    "            prob = exp_rating / np.sum(exp_rating)\n",
    "        \n",
    "            pos_docs = dataset.get_pos_docs(query)\n",
    "            neg_docs = []\n",
    "            for i in range(len(pos_docs)):\n",
    "                while True:\n",
    "                    doc = np.random.choice(can_docs, p=prob)\n",
    "                    if doc not in pos_docs:\n",
    "                        neg_docs.append(doc)\n",
    "                        break\n",
    "            \n",
    "            for i in range(len(pos_docs)):\n",
    "                self.docs_pairs.append((query, pos_docs[i], neg_docs[i]))\n",
    "        \n",
    "    def get_batches(self):\n",
    "        size = len(self.docs_pairs)\n",
    "        cut_off = size // self.batch_size\n",
    "        \n",
    "        for i in range(0, self.batch_size * cut_off, self.batch_size):\n",
    "            batch_pairs = self.docs_pairs[i:i+self.batch_size]\n",
    "            yield np.asarray([self.get_features(p[0], p[1]) for p in batch_pairs]), np.asarray([self.get_features(p[0], p[2]) for p in batch_pairs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are some auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_generator(sess, generator, discriminator, optimizer, dataset):\n",
    "    for query in dataset.get_pos_queries():\n",
    "        pos_docs = dataset.get_pos_docs(query)\n",
    "        can_docs = dataset.get_docs(query)\n",
    "        \n",
    "        can_features = [dataset.get_features(query, doc) for doc in can_docs]\n",
    "        can_score = sess.run(generator.pred_score, feed_dict={generator.pred_data: can_features})\n",
    "        \n",
    "        # softmax for all\n",
    "        exp_rating = np.exp(can_score)\n",
    "        prob = exp_rating / np.sum(exp_rating)\n",
    "        \n",
    "        # sampling\n",
    "        neg_index = np.random.choice(np.arange(len(can_docs)), size=[len(pos_docs)], p=prob)\n",
    "        neg_docs = np.array(can_docs)[neg_index]\n",
    "        \n",
    "        pos_features =  [dataset.get_features(query, doc) for doc in pos_docs]\n",
    "        neg_features = [dataset.get_features(query, doc) for doc in neg_docs]\n",
    "        \n",
    "        neg_reward = sess.run(discriminator.reward,\n",
    "                              feed_dict={discriminator.pos_data: pos_features, discriminator.neg_data: neg_features})\n",
    "            \n",
    "        _ = sess.run(optimizer.g_train_opt, \n",
    "                     feed_dict={generator.pred_data: can_features, generator.sample_index: neg_index, generator.reward: neg_reward})\n",
    "            \n",
    "    return sess.run(generator.opt_loss, \n",
    "                    feed_dict={generator.pred_data: can_features, generator.sample_index: neg_index, generator.reward: neg_reward})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_discriminator(sess, generator, discriminator, optimizer, dataset):\n",
    "    dataset.set_docs_pairs(sess, generator)\n",
    "    \n",
    "    for input_pos, input_neg in dataset.get_batches():\n",
    "        _ = sess.run(optimizer.d_train_opt,\n",
    "                     feed_dict={discriminator.pos_data: input_pos, discriminator.neg_data: input_neg})\n",
    "        \n",
    "    return sess.run(discriminator.opt_loss, \n",
    "                    feed_dict={discriminator.pos_data: input_pos, discriminator.neg_data: input_neg})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def ndcg_at_k(sess, discriminator, dataset, k=5):\n",
    "    ndcg = 0.0\n",
    "    cnt = 0\n",
    "\n",
    "    for query in dataset.get_pos_queries(target='test'):\n",
    "        pos_docs = dataset.get_pos_docs(query, target='test')\n",
    "        pred_docs = dataset.get_docs(query, target='test')\n",
    "\n",
    "        if len(pred_docs) < k:\n",
    "            continue\n",
    "\n",
    "        pred_features = np.asarray([dataset.get_features(query, doc, target='test') for doc in pred_docs])\n",
    "        pred_score = sess.run(discriminator.pred_score, feed_dict={discriminator.pred_data: pred_features})\n",
    "        pred_doc_score = sorted(zip(pred_docs, pred_score), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        dcg = 0.0\n",
    "        for i in range(0, k):\n",
    "            doc, _ = pred_doc_score[i]\n",
    "            if doc in pos_docs:\n",
    "                dcg += (1 / np.log2(i + 2))\n",
    "\n",
    "        n = len(pos_docs) if len(pos_docs) < k else k\n",
    "        idcg = np.sum(np.ones(n) / np.log2(np.arange(2, n + 2)))\n",
    "\n",
    "        ndcg += (dcg / idcg)\n",
    "        cnt += 1\n",
    "\n",
    "    return ndcg / float(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# size of input vector\n",
    "feature_size = 46\n",
    "# size of latent vector\n",
    "hidden_size = 46\n",
    "# keep probability\n",
    "keep_prod = 0.5\n",
    "# learning_rate\n",
    "learning_rate = 0.00001\n",
    "# batch_size\n",
    "batch_size = 8\n",
    "# generator training epochs\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Some objects are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takahish/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "generator = Generator(feature_size, hidden_size, keep_prod)\n",
    "discriminator = Discriminator(feature_size, hidden_size, keep_prod)\n",
    "optimizer = Optimizer(generator, discriminator, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "A generator and discriminator are optimized with dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30... Generator Loss: 4.9770 Discriminator Loss: 0.6435 NDCG@3: 0.0174 NDCG@5: 0.0187 NDCG@10: 0.0249\n",
      "Epoch 2/30... Generator Loss: 4.8975 Discriminator Loss: 0.7134 NDCG@3: 0.0485 NDCG@5: 0.0493 NDCG@10: 0.0612\n",
      "Epoch 3/30... Generator Loss: 5.2394 Discriminator Loss: 0.7249 NDCG@3: 0.1158 NDCG@5: 0.1179 NDCG@10: 0.1270\n",
      "Epoch 4/30... Generator Loss: 4.9916 Discriminator Loss: 0.7441 NDCG@3: 0.1305 NDCG@5: 0.1262 NDCG@10: 0.1355\n",
      "Epoch 5/30... Generator Loss: 5.0673 Discriminator Loss: 0.6937 NDCG@3: 0.1282 NDCG@5: 0.1285 NDCG@10: 0.1386\n",
      "Epoch 6/30... Generator Loss: 5.4678 Discriminator Loss: 0.7267 NDCG@3: 0.1271 NDCG@5: 0.1316 NDCG@10: 0.1469\n",
      "Epoch 7/30... Generator Loss: 5.6698 Discriminator Loss: 0.6418 NDCG@3: 0.1315 NDCG@5: 0.1337 NDCG@10: 0.1524\n",
      "Epoch 8/30... Generator Loss: 4.9896 Discriminator Loss: 0.5203 NDCG@3: 0.1341 NDCG@5: 0.1356 NDCG@10: 0.1580\n",
      "Epoch 9/30... Generator Loss: 4.4342 Discriminator Loss: 0.6565 NDCG@3: 0.1494 NDCG@5: 0.1456 NDCG@10: 0.1648\n",
      "Epoch 10/30... Generator Loss: 4.7965 Discriminator Loss: 0.7414 NDCG@3: 0.1513 NDCG@5: 0.1504 NDCG@10: 0.1733\n",
      "Epoch 11/30... Generator Loss: 5.9252 Discriminator Loss: 0.5982 NDCG@3: 0.1551 NDCG@5: 0.1586 NDCG@10: 0.1806\n",
      "Epoch 12/30... Generator Loss: 3.0153 Discriminator Loss: 0.7675 NDCG@3: 0.1608 NDCG@5: 0.1620 NDCG@10: 0.1846\n",
      "Epoch 13/30... Generator Loss: 5.0974 Discriminator Loss: 0.8457 NDCG@3: 0.1746 NDCG@5: 0.1717 NDCG@10: 0.1892\n",
      "Epoch 14/30... Generator Loss: 4.7626 Discriminator Loss: 0.8694 NDCG@3: 0.1798 NDCG@5: 0.1744 NDCG@10: 0.1929\n",
      "Epoch 15/30... Generator Loss: 3.9240 Discriminator Loss: 0.6030 NDCG@3: 0.1800 NDCG@5: 0.1761 NDCG@10: 0.1985\n",
      "Epoch 16/30... Generator Loss: 5.9999 Discriminator Loss: 0.5459 NDCG@3: 0.1792 NDCG@5: 0.1777 NDCG@10: 0.2011\n",
      "Epoch 17/30... Generator Loss: 4.5736 Discriminator Loss: 0.6075 NDCG@3: 0.1786 NDCG@5: 0.1811 NDCG@10: 0.2005\n",
      "Epoch 18/30... Generator Loss: 5.1476 Discriminator Loss: 0.8375 NDCG@3: 0.1786 NDCG@5: 0.1846 NDCG@10: 0.2142\n",
      "Epoch 19/30... Generator Loss: 2.0694 Discriminator Loss: 0.7811 NDCG@3: 0.1857 NDCG@5: 0.1898 NDCG@10: 0.2145\n",
      "Epoch 20/30... Generator Loss: 4.2366 Discriminator Loss: 0.9697 NDCG@3: 0.1876 NDCG@5: 0.1885 NDCG@10: 0.2168\n",
      "Epoch 21/30... Generator Loss: 4.1992 Discriminator Loss: 0.7159 NDCG@3: 0.2019 NDCG@5: 0.1921 NDCG@10: 0.2254\n",
      "Epoch 22/30... Generator Loss: 3.1851 Discriminator Loss: 0.8611 NDCG@3: 0.1916 NDCG@5: 0.1949 NDCG@10: 0.2242\n",
      "Epoch 23/30... Generator Loss: 2.7664 Discriminator Loss: 0.9353 NDCG@3: 0.1969 NDCG@5: 0.1958 NDCG@10: 0.2220\n",
      "Epoch 24/30... Generator Loss: 6.0334 Discriminator Loss: 0.6384 NDCG@3: 0.1992 NDCG@5: 0.2003 NDCG@10: 0.2205\n",
      "Epoch 25/30... Generator Loss: 2.3052 Discriminator Loss: 0.2148 NDCG@3: 0.1976 NDCG@5: 0.2064 NDCG@10: 0.2234\n",
      "Epoch 26/30... Generator Loss: 4.6441 Discriminator Loss: 1.0977 NDCG@3: 0.2008 NDCG@5: 0.2089 NDCG@10: 0.2275\n",
      "Epoch 27/30... Generator Loss: 3.9517 Discriminator Loss: 0.4302 NDCG@3: 0.2102 NDCG@5: 0.2125 NDCG@10: 0.2314\n",
      "Epoch 28/30... Generator Loss: 2.4171 Discriminator Loss: 0.8308 NDCG@3: 0.2102 NDCG@5: 0.2140 NDCG@10: 0.2329\n",
      "Epoch 29/30... Generator Loss: 5.1407 Discriminator Loss: 0.7144 NDCG@3: 0.2130 NDCG@5: 0.2146 NDCG@10: 0.2366\n",
      "Epoch 30/30... Generator Loss: 4.0255 Discriminator Loss: 0.5159 NDCG@3: 0.2134 NDCG@5: 0.2153 NDCG@10: 0.2371\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(epochs):\n",
    "        g_loss = train_generator(sess, generator, discriminator, optimizer, dataset)\n",
    "        d_loss = train_discriminator(sess, generator, discriminator, optimizer, dataset)\n",
    "        \n",
    "        ndcg_at_3 = ndcg_at_k(sess, discriminator, dataset, k=3)\n",
    "        ndcg_at_5 = ndcg_at_k(sess, discriminator, dataset, k=5)\n",
    "        ndcg_at_10 = ndcg_at_k(sess, discriminator, dataset, k=10)\n",
    "        \n",
    "        print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "              \"Generator Loss: {:.4f}\".format(g_loss), \n",
    "              \"Discriminator Loss: {:.4f}\".format(d_loss),\n",
    "              \"NDCG@3: {:.4f}\".format(ndcg_at_3),\n",
    "              \"NDCG@5: {:.4f}\".format(ndcg_at_5),\n",
    "              \"NDCG@10: {:.4f}\".format(ndcg_at_10))                                   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
